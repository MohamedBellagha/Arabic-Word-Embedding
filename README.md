# Arabic-Word-Embedding
We compiled a large Arabic corpus from various sources to learn word representations. we trained and generated word vectors (embeddings) from the corpus.

# Papers
* Word2vec: Efficient Estimation of Word Representations in Vector Space (2013), T. Mikolov et al. [[pdf]](https://arxiv.org/pdf/1301.3781.pdf)
* GloVe: GloVe: Global Vectors for Word Representation (2014), J. Pennington et al. [[pdf]](https://nlp.stanford.edu/pubs/glove.pdf)
* FastText: Enriching Word Vectors with Subword Information (2016), P. Bojanowski et al. [[pdf]](https://arxiv.org/pdf/1607.04606v1.pdf)
# If you find this work useful in your research, please consider citing:
```sh
@article{Bellagha2020SpeakerNI,
  title={Speaker Naming in TV programs Based on Speaker Role Recognition},
  author={Mohamed Lazhar Bellagha and Mounir Zrigui},
  journal={2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA)},
  year={2020},
  pages={1-8}
}
```
# Set of Arabi text collections
We build a corpus from a set of publicly available text collections. Text contents are mainly news articles based on a local Arabic newspaper and Arabic news broadcast, which include a mixture of different spoken Arabic.

# Preprocessing steps
Named Entity Recognition: linking of named entities using [ALP](http://arabicnlp.pro/arabic-nlp-tool/)
### example: 
* نوري_المالكي: نوري المالكي
* الحرس_الثوري: الحرس الثوري 
* محمد_علي_جعفري: محمد علي جعفري 
* الشرق_الأوسط: الشرق الأوسط
* مجلس_الجامعة_العربية : مجلس الجامعة العربية
* المملكة_العربية_السعودية : المملكة العربية السعودية

# Pre-Trained Word Vectors

| | Link | 
|---|---|
| Word2Vev|[Dawnload](https://mega.nz/folder/R6xQgQqC#4wR_LIlXx14jRfrroFsDNA) |
| FastText|[Dawnload](https://mega.nz/folder/d3hUnQLJ#C0WsvwQ5VgN3VV9V7KWsGA) |
| Glove|[Dawnload](https://mega.nz/folder/wr4UjAwI#e643g99PMsVhYAQ07OMwZQ) |
