# Arabic-Word-Embedding
We compiled a large Arabic corpus from various sources to learn word representations. we trained and generated word vectors (embeddings) from the corpus.

# Papers
* Word2vec: Efficient Estimation of Word Representations in Vector Space (2013), T. Mikolov et al. [[pdf]](https://arxiv.org/pdf/1301.3781.pdf)
* GloVe: GloVe: Global Vectors for Word Representation (2014), J. Pennington et al. [[pdf]](https://nlp.stanford.edu/pubs/glove.pdf)
* FastText: Enriching Word Vectors with Subword Information (2016), P. Bojanowski et al. [[pdf]](https://arxiv.org/pdf/1607.04606v1.pdf)
# Preprocessing steps 
# Pre-Trained Word Vectors
